{"title":"Fifa Radar Plots","markdown":{"yaml":{"title":"Fifa Radar Plots","author":"Ryan Curtis","date":"2023-07-13","categories":["web-scraping","code","analysis"],"image":"image.jpg","jupyter":"python3","format":"html"},"headingText":"THE GOAL","containsRefs":false,"markdown":"\n\n\nFor this assignment, I decided to challenge myself a little bit by not using a dataset that I found on Kaggle. In order to find a new data set, I did the reasonable thing and began my frantic google searches. What I found was that there is a plethora of well formatted data out there, most of it is just online as opposed to a nice and easy excel format. With this project I hope to familarize myself with the In's and Out's of webscraping and maybe clean a dataset and sprinkle in some visualizations just for the fun of it.\n\n## The Dataset\n\nFor this assignment, the first order of buisiness is to decide what to look at. I'm a big soccer fan so I figured I would take a look at something in relation to that. It would be interesting to create some radar plots of players as that is a visualization I have never used before so we will move forward with that in mind as our end goal.\n\nAfter some research, I have decided to use the wonderful resource, \\<sofifa.com\\>. This site is one big table that contains the fifa stats for any football player who is currently in the fifa system, **which is alot**. The first thing we will do, as always, is we are going to prepare our standard visualization and cleaning packages for usage.\n\n```{python}\nimport re\nimport json\nimport requests\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n## WEB-SCRAPING\n\nNow, we are going to import a few more with the goal of ***Web-Scraping***\n\n```{python}\nimport requests\nimport json\nfrom bs4 import BeautifulSoup\n```\n\nNow we want to familiarize ourselves with our website a little bit. Taking a look at the base homepage, we can see that every player has some basic information on display. We can see their Names, Age, Overall, potential, as well as their team and wage they make. Now when we click on an individual player, it gives us a much more thorough breakdown of their individual statistics. We can even see a radar plot, which is exactly what we will be making, however we will be making some adjustments and tweaks to it.\n\nLooking around the site and some of the pages that are on here To begin, I am intested solely in players from the premier league. I need to specify this as my url so I will take this url as my base URL we will use to get general player stats\n\nHowever, in accessing and web-scraping some websites, they have anti-botting software that can deny our request. Running a normal *request.get()* operation will result in our request being denied. To get around this we will be introducing some extra code as compared to a standard web scrape request.\n\n```{python}\nbase_url = \"https://sofifa.com/players?type=all&lg%5B0%5D=13&offset=0\"\n\nreq = requests.Session()\npage = req.get(base_url, headers = {\n  \"User-Agent\": \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)\"\n})\nsoup = BeautifulSoup(page.content, \"html.parser\")\n```\n\nThe main things to note are the usage of the request.session() function, which persists cookies across requests, and the header in *req.get()*, which essentially feeds the site some information to trick it into thinking it is a human making the request as opposed to a program.\n\n## DATA STRUCTURE NAVIGATION\n\nNow that we can actually scrape and get our data, we want to look at our data structure to determine where in it our player data lies. In this case, it stored in a structure called **tbody**, so we want python to grab that class and give us all the information that it holds.\n\n```{python}\nplayer_data = soup.tbody\n```\n\nI'll save you the trouble of looking at the raw soup text, but looking thrugh it we can see it contains all the stats for each player that we could see on the main page, it's just all jumbled up. The next step now is to begin sorting the data into a dictionary that is easily navigable and interpretable.\n\nBy looking through this, we can see that each player's stats start and end with a new class type, **tr**. We can use this information to tell python exactly when to start/stop looking at a player and move onto the next. We can automate this for every player on this list with some loop action\n\n```{python}\nplayer_list=[] #Create empty list to store all data for each player as a seperate entry in list\nplayer_dict={ #Create empty dictionary with all the stats to capture, will later be converted to a Pandas dataframe\n  \"last_name\": [],\n  \"position\": [],\n  \"age\": [],\n  \"overall\": [],\n  \"potential\": [],\n  \"team\": [],\n  \"contract\": [],\n  \"value\": [],\n  \"wage\": [],\n  \"stats\": [],\n  \"id\": []\n} \n\nplayer_list = player_data.find_all(\"tr\") #Seperate each player/stats into a different entry in a list\nfor player in player_list:\n  positions = [] #Create blank list to store positions in case there are multiple positions\n  i = 0\n  \n  player_dict[\"last_name\"].append(player.find(\"div\", class_=\"ellipsis\").get_text())\n  \n  #Some players have multiple positions, join all positions into a list, then get_text and merge list, append to dict\n  positions = player.find_all(class_=re.compile(\"pos\")) #Search through classes to find ones that include \"pos\" \n  while i != len(positions):\n    positions[i] = positions[i].get_text(strip=True)\n    i = i + 1\n  \n  player_dict[\"position\"].append(\" \".join(positions))\n  player_dict[\"age\"].append(player.find(\"td\", class_=\"col col-ae\").get_text())\n  player_dict[\"overall\"].append(int(player.find(\"td\", class_=\"col col-oa col-sort\").get_text()))\n  player_dict[\"potential\"].append(player.find(\"td\", class_=\"col col-pt\").get_text())\n  player_dict[\"team\"].append(player.find(href=re.compile(\"/team/\")).get_text())\n  player_dict[\"contract\"].append(player.find(\"div\", class_=\"sub\").get_text())\n  player_dict[\"value\"].append(player.find(\"td\", class_=\"col col-vl\").get_text())\n  player_dict[\"wage\"].append(player.find(\"td\", class_=\"col col-wg\").get_text())\n  player_dict[\"stats\"].append(player.find(\"span\", class_=\"bp3-tag p\").get_text()) #Not sure what this is for\n  player_dict[\"id\"].append(player.img[\"id\"])\n    \n```\n\nNow that was quite a bit of code. Essentially all we are doing here is creating a list where each entry is a player we have scraped data for, then iterating through the list and saving the data for each player in a dictionary. This dictionary will then turned in to a Pandas dataframe.\n\n```{python}\nplayers_dataframe=pd.DataFrame(player_dict)\nprint(players_dataframe.head(5))\n\n```\n\nThis looks pretty good! There are obviously some things that are slightly wrong with our data frame so lets move onto some cleaning!\n\n## DATA CLEANING\n\nNow that we have (some) of our data into a nice looking data frame, let's work on cleaning it up a little bit, not just for the aesthetics, but also the practicality of having a nice, clean data frame to work with.\n\n```{python}\n#Code to clean up the names\nplayer_name = players_dataframe.last_name #Create series to iterate through\nfor name in player_name:\n  name_split = \"\"\n  if name.find(\".\") != -1: #Only returns -1 if unable to find \".\" character\n    name_split = name.split(\". \", 1)\n    players_dataframe[\"last_name\"] = players_dataframe[\"last_name\"].replace(name, name_split[1]) #Replace original dataframe with updated last name\n  else:\n    pass\n  \n#Code to clean up positions\nplayers_dataframe[[\"position_1\", \"position_2\", \"position_3\"]] = players_dataframe.position.str.split(\" \", expand = True)\nplayers_dataframe = players_dataframe.drop(columns=\"position\")\n\n#Code to clean up contract column; We don't (yet) have a way to determine contract specifics if player is on loan, for now just assign them a value showing they are loan \ncontract_length = players_dataframe.contract\nfor contract in contract_length:\n  fixed_contract = contract.removeprefix(\"\\n\")\n  players_dataframe[\"contract\"] = players_dataframe[\"contract\"].replace(contract, fixed_contract)\n  players_dataframe[[\"contract_start\", \"contract_end\"]] = players_dataframe.contract.str.split(\" ~ \", expand = True)\n  \nplayers_dataframe = players_dataframe.drop(columns=\"contract\")\n\n#Strip currency symbols & suffix from wage and value columns\nplayers_dataframe[\"value\"] = players_dataframe.value.str.removeprefix(\"€\")\nplayers_dataframe[\"wage\"] = players_dataframe.wage.str.removeprefix(\"€\")\n#Remove suffix from value, convert to total amounts \nfor value in players_dataframe.value:\n  replace_value = 0\n  if value.find(\"M\") != -1:\n    replace_value = value.removesuffix(\"M\")\n    replace_value = int(float(replace_value)) * 1000000\n    players_dataframe[\"value\"] = players_dataframe[\"value\"].replace(value, replace_value)\n  elif value.find(\"K\") != -1:\n    replace_value = value.removesuffix(\"K\")\n    replace_value = int(float(replace_value)) * 1000\n    players_dataframe[\"value\"] = players_dataframe[\"value\"].replace(value, replace_value)\n  \n#Same thing but with wages\nfor wage in players_dataframe.wage:\n  replace_wage = 0\n  if wage.find(\"M\") != -1:\n    replace_wage = wage.removesuffix(\"M\")\n    replace_wage = int(float(replace_wage)) * 1000000\n    players_dataframe[\"wage\"] = players_dataframe[\"wage\"].replace(wage, replace_wage)\n  elif wage.find(\"K\") != -1:\n    replace_wage = wage.removesuffix(\"K\")\n    replace_wage = int(float(replace_wage)) * 1000\n    players_dataframe[\"wage\"] = players_dataframe[\"wage\"].replace(wage, replace_wage)\n\nprint(players_dataframe.head(5))\n```\n\nAlright. So now we have gotten some preliminary information on each player. But the real analysis of their playstyle is really only seen in the semantics. We want to get into the nitty gritty for each of these players and see what picture their stats and abilities paint of their playstyle. Let's see if we can can use that unique identifier we saved earlier to access all data for each player.\n\n```{python}\n#Rescrape each player's separate stat page\nfifa_stats = { #Create new dictionary to store the fifa values of players different stats, later merge with original df\n  \"crossing\": [],\n  \"finishing\": [],\n  \"heading_accuracy\": [],\n  \"short_passing\": [],\n  \"volleys\": [],\n  \"dribbling\": [],\n  \"curve\": [],\n  \"fk_accuracy\": [],\n  \"long_passing\": [],\n  \"ball_control\": [],\n  \"acceleration\": [],\n  \"sprint_speed\": [],\n  \"agility\": [],\n  \"reactions\": [],\n  \"balance\": [],\n  \"shot_power\": [],\n  \"jumping\": [],\n  \"stamina\": [],\n  \"strength\": [],\n  \"long_shots\": [],\n  \"aggression\": [],\n  \"interceptions\": [],\n  \"positioning\": [],\n  \"vision\": [],\n  \"penalties\": [],\n  \"composure\": [],\n  \"defensive_awareness\": [],\n  \"standing_tackle\": [],\n  \"sliding_tackle\": [],\n  \"gk_diving\": [],\n  \"gk_handling\": [],\n  \"gk_kicking\": [],\n  \"gk_positioning\": [],\n  \"gk_reflexes\": []\n}\nfor player_id in players_dataframe.id:\n  a = 0\n  #Web-Scrape to get fifa stats from webpage\n  base_url = \"https://sofifa.com/player/\" + player_id\n  req = requests.Session()\n  page = req.get(base_url, headers = {\n    \"User-Agent\": \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)\"\n  })\n  soup = BeautifulSoup(page.content, \"html.parser\")\n  \n  #After successful web scrape, grab all stats from page, scrape away ones we don't need, append values to fifa dict\n  player_stats = soup.find_all(class_=re.compile(\"bp3-tag p p-\"))\n  #print(len(player_stats))\n  if len(player_stats) == 66:\n    del player_stats[0:32] #Remove initial entries in list as we only care about the player stats\n  else:\n    del player_stats[0:31] #Some players have only 65 calls bp3-tag objects, take care of those instances as well\n  for stat in fifa_stats:\n    fifa_stats[stat].append(int(player_stats[a].get_text()))\n    a = a + 1\n  fifa_stats_df=pd.DataFrame(fifa_stats)\n\n#Now we can join together these two dataframes\nplayer_stats_df = pd.concat([players_dataframe, fifa_stats_df], axis=1, join=\"inner\")\nprint(player_stats_df.head(5))\n```\n\nLooking good! Now we have a nicely sized dataframe that stores all data and stats of our webscraped players. Since this is mostly cleaned and ready for usage, we can now move onto creating our radar plots!\n\n## Visualizations\n\nJust for the fun of it we'll make a correlation matrix to see what stats most heavily influence a player's overall rating because I think that it would be interesting to see if this is simply an average of all statistics for players or if the overall is more dependent on certain stats.\n\n```{python}\n#Convert overall to int, add to player stats df, create correlation heatmap \ncorrmat = fifa_stats_df.join(players_dataframe[\"overall\"]).corr()\nsns.heatmap(corrmat, square=True)\n\n```\n\nFor our next little project, we will be moving onto the spider plots.\n\nI liked the idea of grouping the players' stats into categories, as a radar plot with as many points as we have columns would not be a great visualization. I decided to group the stats into different categories based on their function. Once we make those categories, we can then simply take the average of the datapoints in them and use that as one of values to plot on the Radar plot.\n\n```{python}\n#for usage later in dataframe\nnames = [\"attacking\", \"passing\", \"physicality\", \"pace\", \"dribbling\", \"mentality\", \"defending\", \"goalkeeping\"]\naverage = [0, 0, 0, 0, 0, 0, 0, 0]\n\n#create loop to make categories for each player\nfor index, row in player_stats_df.iterrows():\n  \n  #First assign values to a list, then map them as INT, then get mean. Repeat for every point of radar plot\n  attacking_col = [row[\"finishing\"], row[\"heading_accuracy\"], row[\"volleys\"], row[\"shot_power\"], row[\"long_shots\"]]\n  attacking_col = list(map(int, attacking_col))\n  average[0] = np.mean(attacking_col)\n  \n  passing_col = [row[\"short_passing\"], row[\"long_passing\"], row[\"crossing\"], row[\"fk_accuracy\"]]\n  passing_col = list(map(int, passing_col))\n  average[1] = np.mean(passing_col)\n  \n  physicality_col = [row[\"reactions\"], row[\"balance\"], row[\"strength\"], row[\"jumping\"]]\n  physicality_col = list(map(int, physicality_col))\n  average[2] = np.mean(physicality_col)\n  \n  pace_col = [row[\"acceleration\"], row[\"sprint_speed\"], row[\"agility\"]]\n  pace_col = list(map(int, pace_col))\n  average[3] = np.mean(pace_col)\n  \n  dribbling_col = [row[\"dribbling\"], row[\"ball_control\"]]\n  dribbling_col = list(map(int, dribbling_col))\n  average[4] = np.mean(dribbling_col)\n  \n  mentality_col = [row[\"aggression\"], row[\"positioning\"], row[\"vision\"], row[\"composure\"]]\n  mentality_col = list(map(int, mentality_col))\n  average[5] = np.mean(mentality_col)\n  \n  defending_col = [row[\"interceptions\"], row[\"defensive_awareness\"], row[\"standing_tackle\"], row[\"sliding_tackle\"]]\n  defending_col = list(map(int, defending_col))\n  average[6] = np.mean(defending_col)\n  \n  goalkeeping_col = [row[\"gk_diving\"], row[\"gk_handling\"], row[\"gk_kicking\"], row[\"gk_positioning\"], row[\"gk_reflexes\"]]\n  goalkeeping_col = list(map(int, goalkeeping_col))\n  average[7] = np.mean(goalkeeping_col)\n  \n  #Create dataframe to make radar plot\n  data_dict = {\"names\": names, \"average\": average} \n  df = pd.DataFrame(data_dict)\n  \n  #Create and show graphs for every player\n  fig = px.line_polar(df, r=\"average\", theta =\"names\", line_close=True)\n  fig.update_traces(fill=\"toself\")\n  print(\"Player:\", row[\"last_name\"],\",\", \"Overall:\", row[\"overall\"],\",\",  \"Position:\", row[\"position_1\"])\n  fig.show()\n  \n```\n\nAnd there we go! They're not the most informative or beatiful visualizations, but they were largely just for practice in webscraping anyways. This was an excellent project that familiarized myself with the ins and outs of web scraping.\n","srcMarkdownNoYaml":"\n\n# THE GOAL\n\nFor this assignment, I decided to challenge myself a little bit by not using a dataset that I found on Kaggle. In order to find a new data set, I did the reasonable thing and began my frantic google searches. What I found was that there is a plethora of well formatted data out there, most of it is just online as opposed to a nice and easy excel format. With this project I hope to familarize myself with the In's and Out's of webscraping and maybe clean a dataset and sprinkle in some visualizations just for the fun of it.\n\n## The Dataset\n\nFor this assignment, the first order of buisiness is to decide what to look at. I'm a big soccer fan so I figured I would take a look at something in relation to that. It would be interesting to create some radar plots of players as that is a visualization I have never used before so we will move forward with that in mind as our end goal.\n\nAfter some research, I have decided to use the wonderful resource, \\<sofifa.com\\>. This site is one big table that contains the fifa stats for any football player who is currently in the fifa system, **which is alot**. The first thing we will do, as always, is we are going to prepare our standard visualization and cleaning packages for usage.\n\n```{python}\nimport re\nimport json\nimport requests\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n## WEB-SCRAPING\n\nNow, we are going to import a few more with the goal of ***Web-Scraping***\n\n```{python}\nimport requests\nimport json\nfrom bs4 import BeautifulSoup\n```\n\nNow we want to familiarize ourselves with our website a little bit. Taking a look at the base homepage, we can see that every player has some basic information on display. We can see their Names, Age, Overall, potential, as well as their team and wage they make. Now when we click on an individual player, it gives us a much more thorough breakdown of their individual statistics. We can even see a radar plot, which is exactly what we will be making, however we will be making some adjustments and tweaks to it.\n\nLooking around the site and some of the pages that are on here To begin, I am intested solely in players from the premier league. I need to specify this as my url so I will take this url as my base URL we will use to get general player stats\n\nHowever, in accessing and web-scraping some websites, they have anti-botting software that can deny our request. Running a normal *request.get()* operation will result in our request being denied. To get around this we will be introducing some extra code as compared to a standard web scrape request.\n\n```{python}\nbase_url = \"https://sofifa.com/players?type=all&lg%5B0%5D=13&offset=0\"\n\nreq = requests.Session()\npage = req.get(base_url, headers = {\n  \"User-Agent\": \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)\"\n})\nsoup = BeautifulSoup(page.content, \"html.parser\")\n```\n\nThe main things to note are the usage of the request.session() function, which persists cookies across requests, and the header in *req.get()*, which essentially feeds the site some information to trick it into thinking it is a human making the request as opposed to a program.\n\n## DATA STRUCTURE NAVIGATION\n\nNow that we can actually scrape and get our data, we want to look at our data structure to determine where in it our player data lies. In this case, it stored in a structure called **tbody**, so we want python to grab that class and give us all the information that it holds.\n\n```{python}\nplayer_data = soup.tbody\n```\n\nI'll save you the trouble of looking at the raw soup text, but looking thrugh it we can see it contains all the stats for each player that we could see on the main page, it's just all jumbled up. The next step now is to begin sorting the data into a dictionary that is easily navigable and interpretable.\n\nBy looking through this, we can see that each player's stats start and end with a new class type, **tr**. We can use this information to tell python exactly when to start/stop looking at a player and move onto the next. We can automate this for every player on this list with some loop action\n\n```{python}\nplayer_list=[] #Create empty list to store all data for each player as a seperate entry in list\nplayer_dict={ #Create empty dictionary with all the stats to capture, will later be converted to a Pandas dataframe\n  \"last_name\": [],\n  \"position\": [],\n  \"age\": [],\n  \"overall\": [],\n  \"potential\": [],\n  \"team\": [],\n  \"contract\": [],\n  \"value\": [],\n  \"wage\": [],\n  \"stats\": [],\n  \"id\": []\n} \n\nplayer_list = player_data.find_all(\"tr\") #Seperate each player/stats into a different entry in a list\nfor player in player_list:\n  positions = [] #Create blank list to store positions in case there are multiple positions\n  i = 0\n  \n  player_dict[\"last_name\"].append(player.find(\"div\", class_=\"ellipsis\").get_text())\n  \n  #Some players have multiple positions, join all positions into a list, then get_text and merge list, append to dict\n  positions = player.find_all(class_=re.compile(\"pos\")) #Search through classes to find ones that include \"pos\" \n  while i != len(positions):\n    positions[i] = positions[i].get_text(strip=True)\n    i = i + 1\n  \n  player_dict[\"position\"].append(\" \".join(positions))\n  player_dict[\"age\"].append(player.find(\"td\", class_=\"col col-ae\").get_text())\n  player_dict[\"overall\"].append(int(player.find(\"td\", class_=\"col col-oa col-sort\").get_text()))\n  player_dict[\"potential\"].append(player.find(\"td\", class_=\"col col-pt\").get_text())\n  player_dict[\"team\"].append(player.find(href=re.compile(\"/team/\")).get_text())\n  player_dict[\"contract\"].append(player.find(\"div\", class_=\"sub\").get_text())\n  player_dict[\"value\"].append(player.find(\"td\", class_=\"col col-vl\").get_text())\n  player_dict[\"wage\"].append(player.find(\"td\", class_=\"col col-wg\").get_text())\n  player_dict[\"stats\"].append(player.find(\"span\", class_=\"bp3-tag p\").get_text()) #Not sure what this is for\n  player_dict[\"id\"].append(player.img[\"id\"])\n    \n```\n\nNow that was quite a bit of code. Essentially all we are doing here is creating a list where each entry is a player we have scraped data for, then iterating through the list and saving the data for each player in a dictionary. This dictionary will then turned in to a Pandas dataframe.\n\n```{python}\nplayers_dataframe=pd.DataFrame(player_dict)\nprint(players_dataframe.head(5))\n\n```\n\nThis looks pretty good! There are obviously some things that are slightly wrong with our data frame so lets move onto some cleaning!\n\n## DATA CLEANING\n\nNow that we have (some) of our data into a nice looking data frame, let's work on cleaning it up a little bit, not just for the aesthetics, but also the practicality of having a nice, clean data frame to work with.\n\n```{python}\n#Code to clean up the names\nplayer_name = players_dataframe.last_name #Create series to iterate through\nfor name in player_name:\n  name_split = \"\"\n  if name.find(\".\") != -1: #Only returns -1 if unable to find \".\" character\n    name_split = name.split(\". \", 1)\n    players_dataframe[\"last_name\"] = players_dataframe[\"last_name\"].replace(name, name_split[1]) #Replace original dataframe with updated last name\n  else:\n    pass\n  \n#Code to clean up positions\nplayers_dataframe[[\"position_1\", \"position_2\", \"position_3\"]] = players_dataframe.position.str.split(\" \", expand = True)\nplayers_dataframe = players_dataframe.drop(columns=\"position\")\n\n#Code to clean up contract column; We don't (yet) have a way to determine contract specifics if player is on loan, for now just assign them a value showing they are loan \ncontract_length = players_dataframe.contract\nfor contract in contract_length:\n  fixed_contract = contract.removeprefix(\"\\n\")\n  players_dataframe[\"contract\"] = players_dataframe[\"contract\"].replace(contract, fixed_contract)\n  players_dataframe[[\"contract_start\", \"contract_end\"]] = players_dataframe.contract.str.split(\" ~ \", expand = True)\n  \nplayers_dataframe = players_dataframe.drop(columns=\"contract\")\n\n#Strip currency symbols & suffix from wage and value columns\nplayers_dataframe[\"value\"] = players_dataframe.value.str.removeprefix(\"€\")\nplayers_dataframe[\"wage\"] = players_dataframe.wage.str.removeprefix(\"€\")\n#Remove suffix from value, convert to total amounts \nfor value in players_dataframe.value:\n  replace_value = 0\n  if value.find(\"M\") != -1:\n    replace_value = value.removesuffix(\"M\")\n    replace_value = int(float(replace_value)) * 1000000\n    players_dataframe[\"value\"] = players_dataframe[\"value\"].replace(value, replace_value)\n  elif value.find(\"K\") != -1:\n    replace_value = value.removesuffix(\"K\")\n    replace_value = int(float(replace_value)) * 1000\n    players_dataframe[\"value\"] = players_dataframe[\"value\"].replace(value, replace_value)\n  \n#Same thing but with wages\nfor wage in players_dataframe.wage:\n  replace_wage = 0\n  if wage.find(\"M\") != -1:\n    replace_wage = wage.removesuffix(\"M\")\n    replace_wage = int(float(replace_wage)) * 1000000\n    players_dataframe[\"wage\"] = players_dataframe[\"wage\"].replace(wage, replace_wage)\n  elif wage.find(\"K\") != -1:\n    replace_wage = wage.removesuffix(\"K\")\n    replace_wage = int(float(replace_wage)) * 1000\n    players_dataframe[\"wage\"] = players_dataframe[\"wage\"].replace(wage, replace_wage)\n\nprint(players_dataframe.head(5))\n```\n\nAlright. So now we have gotten some preliminary information on each player. But the real analysis of their playstyle is really only seen in the semantics. We want to get into the nitty gritty for each of these players and see what picture their stats and abilities paint of their playstyle. Let's see if we can can use that unique identifier we saved earlier to access all data for each player.\n\n```{python}\n#Rescrape each player's separate stat page\nfifa_stats = { #Create new dictionary to store the fifa values of players different stats, later merge with original df\n  \"crossing\": [],\n  \"finishing\": [],\n  \"heading_accuracy\": [],\n  \"short_passing\": [],\n  \"volleys\": [],\n  \"dribbling\": [],\n  \"curve\": [],\n  \"fk_accuracy\": [],\n  \"long_passing\": [],\n  \"ball_control\": [],\n  \"acceleration\": [],\n  \"sprint_speed\": [],\n  \"agility\": [],\n  \"reactions\": [],\n  \"balance\": [],\n  \"shot_power\": [],\n  \"jumping\": [],\n  \"stamina\": [],\n  \"strength\": [],\n  \"long_shots\": [],\n  \"aggression\": [],\n  \"interceptions\": [],\n  \"positioning\": [],\n  \"vision\": [],\n  \"penalties\": [],\n  \"composure\": [],\n  \"defensive_awareness\": [],\n  \"standing_tackle\": [],\n  \"sliding_tackle\": [],\n  \"gk_diving\": [],\n  \"gk_handling\": [],\n  \"gk_kicking\": [],\n  \"gk_positioning\": [],\n  \"gk_reflexes\": []\n}\nfor player_id in players_dataframe.id:\n  a = 0\n  #Web-Scrape to get fifa stats from webpage\n  base_url = \"https://sofifa.com/player/\" + player_id\n  req = requests.Session()\n  page = req.get(base_url, headers = {\n    \"User-Agent\": \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.3; Win64; x64)\"\n  })\n  soup = BeautifulSoup(page.content, \"html.parser\")\n  \n  #After successful web scrape, grab all stats from page, scrape away ones we don't need, append values to fifa dict\n  player_stats = soup.find_all(class_=re.compile(\"bp3-tag p p-\"))\n  #print(len(player_stats))\n  if len(player_stats) == 66:\n    del player_stats[0:32] #Remove initial entries in list as we only care about the player stats\n  else:\n    del player_stats[0:31] #Some players have only 65 calls bp3-tag objects, take care of those instances as well\n  for stat in fifa_stats:\n    fifa_stats[stat].append(int(player_stats[a].get_text()))\n    a = a + 1\n  fifa_stats_df=pd.DataFrame(fifa_stats)\n\n#Now we can join together these two dataframes\nplayer_stats_df = pd.concat([players_dataframe, fifa_stats_df], axis=1, join=\"inner\")\nprint(player_stats_df.head(5))\n```\n\nLooking good! Now we have a nicely sized dataframe that stores all data and stats of our webscraped players. Since this is mostly cleaned and ready for usage, we can now move onto creating our radar plots!\n\n## Visualizations\n\nJust for the fun of it we'll make a correlation matrix to see what stats most heavily influence a player's overall rating because I think that it would be interesting to see if this is simply an average of all statistics for players or if the overall is more dependent on certain stats.\n\n```{python}\n#Convert overall to int, add to player stats df, create correlation heatmap \ncorrmat = fifa_stats_df.join(players_dataframe[\"overall\"]).corr()\nsns.heatmap(corrmat, square=True)\n\n```\n\nFor our next little project, we will be moving onto the spider plots.\n\nI liked the idea of grouping the players' stats into categories, as a radar plot with as many points as we have columns would not be a great visualization. I decided to group the stats into different categories based on their function. Once we make those categories, we can then simply take the average of the datapoints in them and use that as one of values to plot on the Radar plot.\n\n```{python}\n#for usage later in dataframe\nnames = [\"attacking\", \"passing\", \"physicality\", \"pace\", \"dribbling\", \"mentality\", \"defending\", \"goalkeeping\"]\naverage = [0, 0, 0, 0, 0, 0, 0, 0]\n\n#create loop to make categories for each player\nfor index, row in player_stats_df.iterrows():\n  \n  #First assign values to a list, then map them as INT, then get mean. Repeat for every point of radar plot\n  attacking_col = [row[\"finishing\"], row[\"heading_accuracy\"], row[\"volleys\"], row[\"shot_power\"], row[\"long_shots\"]]\n  attacking_col = list(map(int, attacking_col))\n  average[0] = np.mean(attacking_col)\n  \n  passing_col = [row[\"short_passing\"], row[\"long_passing\"], row[\"crossing\"], row[\"fk_accuracy\"]]\n  passing_col = list(map(int, passing_col))\n  average[1] = np.mean(passing_col)\n  \n  physicality_col = [row[\"reactions\"], row[\"balance\"], row[\"strength\"], row[\"jumping\"]]\n  physicality_col = list(map(int, physicality_col))\n  average[2] = np.mean(physicality_col)\n  \n  pace_col = [row[\"acceleration\"], row[\"sprint_speed\"], row[\"agility\"]]\n  pace_col = list(map(int, pace_col))\n  average[3] = np.mean(pace_col)\n  \n  dribbling_col = [row[\"dribbling\"], row[\"ball_control\"]]\n  dribbling_col = list(map(int, dribbling_col))\n  average[4] = np.mean(dribbling_col)\n  \n  mentality_col = [row[\"aggression\"], row[\"positioning\"], row[\"vision\"], row[\"composure\"]]\n  mentality_col = list(map(int, mentality_col))\n  average[5] = np.mean(mentality_col)\n  \n  defending_col = [row[\"interceptions\"], row[\"defensive_awareness\"], row[\"standing_tackle\"], row[\"sliding_tackle\"]]\n  defending_col = list(map(int, defending_col))\n  average[6] = np.mean(defending_col)\n  \n  goalkeeping_col = [row[\"gk_diving\"], row[\"gk_handling\"], row[\"gk_kicking\"], row[\"gk_positioning\"], row[\"gk_reflexes\"]]\n  goalkeeping_col = list(map(int, goalkeeping_col))\n  average[7] = np.mean(goalkeeping_col)\n  \n  #Create dataframe to make radar plot\n  data_dict = {\"names\": names, \"average\": average} \n  df = pd.DataFrame(data_dict)\n  \n  #Create and show graphs for every player\n  fig = px.line_polar(df, r=\"average\", theta =\"names\", line_close=True)\n  fig.update_traces(fill=\"toself\")\n  print(\"Player:\", row[\"last_name\"],\",\", \"Overall:\", row[\"overall\"],\",\",  \"Position:\", row[\"position_1\"])\n  fig.show()\n  \n```\n\nAnd there we go! They're not the most informative or beatiful visualizations, but they were largely just for practice in webscraping anyways. This was an excellent project that familiarized myself with the ins and outs of web scraping.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":{"light":"cosmo","dark":"darkly"},"title-block-banner":true,"title":"Fifa Radar Plots","author":"Ryan Curtis","date":"2023-07-13","categories":["web-scraping","code","analysis"],"image":"image.jpg","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}